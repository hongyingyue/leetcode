{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f226df",
   "metadata": {},
   "source": [
    "## Linear Regression Using Gradient Descent\n",
    "Easy\n",
    "Machine Learning\n",
    "\n",
    "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Clarify:\n",
    "- what type of GD to use: all samples per iteration?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix computation\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, \n",
    "                                       alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    y = y.reshape(-1, 1)  # Ensure column vector\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (X.T @ (y_pred - y)) / m\n",
    "        theta -= alpha * gradient\n",
    "\n",
    "    theta = np.round(theta, 4)\n",
    "    return [val.item() for val in theta]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956df895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "def update_w_and_b(spendings, sales, w, b, alpha):\n",
    "    dl_dw = 0.0\n",
    "    dl_db = 0.0\n",
    "    N = len(spendings)\n",
    "    for i in range(N):\n",
    "        dl_dw += -2*spendings[i]*(sales[i] - (w*spendings[i] + b))\n",
    "        dl_db += -2*(sales[i] - (w*spendings[i] + b))\n",
    "        \n",
    "    # update w and b\n",
    "    w = w - (1/float(N))*dl_dw*alpha\n",
    "    b = b - (1/float(N))*dl_db*alpha\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4792b5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1107, 0.9513]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "print(linear_regression_gradient_descent(X, y, alpha, iterations))\n",
    "# Output: np.array([0.1107, 0.9513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3c225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(X, 0, 1, axis=1)\n",
    "np.delete(X, 0, axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd7e630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m, n = X.shape\n",
    "theta = np.zeros((n, 1))\n",
    "\n",
    "y_pred = (X @ theta).ravel()\n",
    "theta += ((y - y_pred) @ X * alpha).reshape(n, 1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83a426",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent Variants with MSE Loss\n",
    "Medium\n",
    "Machine Learning\n",
    "\n",
    "\n",
    "In this problem, you need to implement a single function that can perform three variants of gradient descent Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini Batch Gradient Descent using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use. \n",
    "\n",
    "Note: **Do not shuffle** the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "723883f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nClarify:\\n- Include intercept in the paramenters?\\n- Mini-batch: don't shuffle -> use the data in the original order?\\n- batch_size: can we assume it can be divided by the sample size?\\n\\nMSE = mean[(y_pred - y_true) ^ 2]\\ngradient = 2 * X @ (y_pred - y_true) / batch_size \\ntheta = theta - lr * gradient\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Clarify:\n",
    "- Include intercept in the paramenters?\n",
    "- Mini-batch: don't shuffle -> use the data in the original order?\n",
    "- batch_size: can we assume it can be divided by the sample size?\n",
    "\n",
    "MSE = mean[(y_pred - y_true) ^ 2]\n",
    "gradient = 2 * X @ (y_pred - y_true) / batch_size \n",
    "theta = theta - lr * gradient\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b462cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch', shuffle=True):\n",
    "    \"\"\"\n",
    "    X: (m, n) input features\n",
    "    y: (m,) target values\n",
    "    weights: (n,) initial weights\n",
    "    learning_rate: float\n",
    "    n_iterations: int\n",
    "    batch_size: int\n",
    "    method: 'batch', 'stochastic', or 'mini_batch'\n",
    "    shuffle: whether to shuffle data each epoch (default True)\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    if method == 'batch':\n",
    "        batch_size = m\n",
    "    elif method == 'stochastic':\n",
    "        batch_size = 1\n",
    "    elif method == 'mini_batch':\n",
    "        assert 1 < batch_size < m, \"Mini-batch size must be between 1 and m\"\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # if shuffle:\n",
    "        #     indices = np.random.permutation(m)\n",
    "        # else:\n",
    "        indices = np.arange(m)\n",
    "\n",
    "        for start in range(0, m, batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_idx = indices[start : end]\n",
    "\n",
    "            X_batch = X[batch_idx]\n",
    "            y_true = y[batch_idx]\n",
    "            y_pred = X_batch @ weights\n",
    "            \n",
    "            gradient = 2 * X_batch.T @ (y_pred - y_true) / batch_size\n",
    "            weights -= learning_rate * gradient\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.10334065 0.68329431]\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 100\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(X.shape[1])\n",
    "\n",
    "# Test Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
    "print(final_weights)\n",
    "\n",
    "# Test Stochastic Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
    "print(final_weights)\n",
    "\n",
    "# Test Mini-Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac375f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
